# 测试数据

从BIIRD-CRITIC摘出的相关测试用例，用于调试：

----

**数据库版本：MySQL 8.4.0**

**数据库ID：debit_card_specializing**

```shell
python diag.py --sql "SET @current_time = TIMESTAMP('2022-12-01 12:40:00');WITH cte AS (SELECT slot, SUM(total) OVER(ORDER BY slot) AS total, total AS rowtotal FROM sales WHERE slot < @current_time ORDER BY slot DESC LIMIT 1) SELECT total - (30 - TIMESTAMPDIFF(MINUTE, slot, @current_time))/30 * rowtotal AS total FROM cte" --tables "customers" --preprocess-sql "CREATE TABLE sales (id INT, slot TIMESTAMP, total INT);INSERT INTO sales VALUES(1, '2022-12-01T12:00', 100), (2, '2022-12-01T12:30', 150), (3, '2022-12-01T13:00', 200);CREATE INDEX idx_test ON sales (slot);" --clean-up-sql "SET @current_time := NULL;DROP TABLE sales;" --user-prompt "Suppose we have a transactions data table within an e-commerce platform that records purchases made by various customers at different gas stations. The table 'sales' is like |id|slot|total|
There's an index on slot already. I want to sum the total up to the current moment in time (EDIT: WASN\'T CLEAR INITIALLY, I WILL PROVIDE A LOWER SLOT BOUND, SO THE SUM WILL BE OVER SOME NUMBER OF DAYS/WEEKS, NOT OVER FULL TABLE). Let\'s say the time is currently 2022-12-01T12:45. If I run select * from my_table where slot < CURRENT_TIMESTAMP(), then I get back records 1 and 2. However, in my data, the records represent forecasted sales within a time slot. I want to find the forecasts as of 2022-12-01T12:45, and so I want to find the proportion of the half hour slot of record 2 that has elapsed, and return that proportion of the total. As of 2022-12-01T12:45 (assuming minute granularity), 50% of row 2 has elapsed, so I would expect the total to return as 150 / 2 = 75. My current query works, but is slow. What are some ways I can optimise this, or other approaches I can take? Also, how can we extend this solution to be generalised to any interval frequency? Maybe tomorrow we change our forecasting model and the data comes in sporadically. The hardcoded 30 would not work in that case.The platform tracks the sales forecasts for gas products, which are recorded in half-hour time slots. Due to recent platform updates, users want to calculate the total forecasted sales up to the current moment in time, taking into account the proportion of the current half-hour slot that has elapsed."
```

```shell
python diag.py --sql "WITH RECURSIVE `timestamps`(`timestamp`) AS ( SELECT ( SELECT FROM_UNIXTIME( UNIX_TIMESTAMP(MIN(`timestamp`)) - MOD(UNIX_TIMESTAMP(MIN(`timestamp`)), 1800)) FROM `statuses`) UNION ALL SELECT DATE_ADD(`timestamp`, INTERVAL 30 MINUTE) FROM `timestamps` WHERE `timestamp` < ( SELECT FROM_UNIXTIME( UNIX_TIMESTAMP(MAX(`timestamp`)) - MOD(UNIX_TIMESTAMP(MAX(`timestamp`)), 1800)) FROM `statuses`)) SELECT `t`.`timestamp`, `s`.`status` FROM `timestamps` AS `t` LEFT OUTER JOIN `statuses` AS `s` ON `t`.`timestamp` = FROM_UNIXTIME( UNIX_TIMESTAMP(`s`.`timestamp`) - MOD(UNIX_TIMESTAMP(`s`.`timestamp`), 1800)) ORDER BY `t`.`timestamp` ASC;" --tables "customers" --preprocess-sql "CREATE TABLE IF NOT EXISTS `statuses` (`timestamp` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, `status` INT NOT NULL DEFAULT '0', PRIMARY KEY (`timestamp`) ); INSERT IGNORE INTO `statuses` (`timestamp`, `status`) VALUES ('2023-01-01 00:03:34', '164850'), ('2023-01-01 00:31:23', '794088'), ('2023-01-01 03:31:28', '686754'), ('2023-01-01 04:01:15', '684711'), ('2023-01-01 05:31:35', '116777'), ('2023-01-01 06:01:52', '469332'), ('2023-01-01 06:31:55', '816300'), ('2023-01-01 08:33:53', '309583'), ('2023-01-01 09:03:54', '847976'), ('2023-01-01 09:31:33', '812517');" --clean-up-sql "DROP TABLE IF EXISTS statuses;" --user-prompt "Imagine a scenario where a script logs the transaction data into the `status` table every thirty minutes, but due to various reasons, some intervals might be skipped, leading to gaps in the data. The goal is to fetch all transactions with timestamps rounded to the closest half-hour interval and include empty rows (all fields except for the timestamp should be null) for any intervals where no transaction data is available. The table structure, data, and script cannot be altered. The only solution I could come up with that yields the desired result doesn't scale. Can you help optimize my sql?"
```

----

**数据库版本：PostgreSQL 14.1**

**数据库ID：financial**

```shell
python diag.py --sql "SELECT t.trans_id,       t.account_id,       t.date,       t.type,       t.amount  FROM trans t  JOIN account a    ON t.account_id = a.account_id WHERE a.district_id = 18   AND t.bank = 'AB'  AND t.type IN ('PRIJEM', 'VYDAJ');" --tables "trans" "account" --user-prompt "We have a large transaction table in our financial database with over 180 million rows and 20 GB in size. The table is structured to store detailed transaction records for various accounts. We are running a query to retrieve specific transactions based on a list of account IDs, a specific bank, and a range of transaction types. The query is taking an unexpectedly long time to execute when the shared buffers are cold, around 9 seconds, but only 25 ms when the data is cached. We suspect that the query planner is not choosing the most optimal execution plan. We have tried adding a covering index and forcing a Bitmap Heap Scan, but we would like to understand why the planner is not making the best choice and find a more permanent solution to improve performance to around 1-2 seconds."
```

```shell
python diag.py --sql "select * from trans where date_part('month', 'date') = date_part('month', now()) and date_part('day', 'date') = date_part('day', now()) order by 'date' desc;" --tables "trans" --preprocess-sql "CREATE INDEX ix1 ON trans (EXTRACT(MONTH FROM date), EXTRACT(DAY FROM date));" --clean-up-sql "drop index if exists ix1;" --user-prompt "I am trying to speed up a PostgreSQL query to find previous transactions on the same day of the year from the 'trans' table. My current query is as follows:\nsql \nselect * from trans \nwhere date_part('month', date) = date_part('month', now()) \nand date_part('day', date) = date_part('day', now()) \norder by date desc; \n\nThis query works but is running much slower than desired. Is there a better approach for comparing the current month and day?\nThe data is time-series in nature, and I am using PostgreSQL as the database."
```

----